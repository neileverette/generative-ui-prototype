---
phase: 30-data-sync-client
plan: 01
type: execute
---

<objective>
Add HTTP client code to the scraper that POSTs usage data to the EC2 endpoint after each successful scrape.

Purpose: Enable the widget to work from anywhere by syncing scraped data to EC2 instead of only saving locally.
Output: Scraper syncs data to EC2 after each successful scrape with proper error handling and retry logic.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-phase.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

**Current Architecture:**
- Scraper runs locally on laptop (authenticated Console session)
- Writes data to local JSON file (`server/claude-scraper/usage-data.json`)
- Widget reads from local file via server endpoint
- Problem: Widget only works on same machine as scraper

**Phase 29 Completed:**
- EC2 API endpoint: POST /api/claude/console-usage
- Authentication: X-API-Key header with CLAUDE_SYNC_API_KEY
- Validation: lastUpdated (ISO 8601), percentageUsed (0-100), resetsIn (non-empty)
- Supports partial data (optional currentSession/weeklyLimits)
- Storage: console-usage-synced.json (separate from local scraper file)

**Key Files:**
@server/claude-scraper/auto-scraper.ts - Current scraper implementation (208 lines)
@server/claude-scraper/retry-strategy.ts - Existing retry logic with exponential backoff (224 lines)
@server/claude-console-sync-types.ts - Type definitions for sync (ConsoleUsageDataSync, SyncResponse)
@server/claude-scraper/scrape.ts - Data extraction with ConsoleUsageData type

**Tech Stack Available:**
- Node.js v22.22.0 (native fetch support)
- TypeScript
- Existing RetryStrategy class (exponential backoff, circuit breaker)
- Environment variables: CLAUDE_SYNC_API_KEY, CLAUDE_SYNC_URL

**Constraining Decisions:**
- Phase 23: Partial data is treated as success (don't fail sync on partial scrapes)
- Phase 29: Use X-API-Key header for authentication
- Phase 29: Endpoint expects ConsoleUsageDataSync format (not ConsoleUsageData)
- RetryStrategy: Network errors retry with exponential backoff, fatal errors exit immediately
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create sync client module</name>
  <files>server/claude-scraper/sync-client.ts</files>
  <action>
Create TypeScript module that POSTs usage data to EC2 endpoint:

**Function signature:**
```typescript
export async function syncToEC2(data: ConsoleUsageData): Promise<SyncResponse>
```

**Implementation:**
1. Read environment variables:
   - CLAUDE_SYNC_URL (EC2 endpoint URL, default: http://localhost:4000/api/claude/console-usage)
   - CLAUDE_SYNC_API_KEY (required for authentication)
2. If CLAUDE_SYNC_API_KEY is not set, log warning and return early with success: false
3. Transform ConsoleUsageData to ConsoleUsageDataSync format:
   - Keep currentSession, weeklyLimits, isPartial, extractionErrors as-is
   - Use existing lastUpdated field
4. Use native fetch with POST method:
   - Headers: Content-Type: application/json, X-API-Key: CLAUDE_SYNC_API_KEY
   - Body: JSON.stringify(syncData)
   - Timeout: 10 seconds (AbortController)
5. Handle responses:
   - 2xx: Parse as SyncResponse, return it
   - 401: Throw Error('Sync authentication failed: Invalid API key')
   - 400: Throw Error('Sync validation failed: ' + response error message)
   - Network errors: Throw Error('Sync network error: ' + error message)
6. Add type imports from sync-types and scrape.ts

**What to avoid:**
- Don't use axios or node-fetch (use native fetch - Node v22 has it)
- Don't retry here (auto-scraper handles retries via RetryStrategy)
- Don't log full data payloads (contains usage info - log summary only)
- Don't swallow errors (throw them for auto-scraper to handle)
  </action>
  <verify>
TypeScript compiles without errors: npx tsc --noEmit
Module exports syncToEC2 function with correct signature
  </verify>
  <done>
sync-client.ts exists with syncToEC2 function
Handles all response codes (2xx, 401, 400, network errors)
Uses native fetch with 10s timeout
Returns SyncResponse or throws descriptive errors
  </done>
</task>

<task type="auto">
  <name>Task 2: Integrate sync into auto-scraper</name>
  <files>server/claude-scraper/auto-scraper.ts</files>
  <action>
Add sync client integration after successful scrapes:

**Location:** After line 95 (after logging scrape completion) in the success block

**Implementation:**
1. Import syncToEC2 from ./sync-client.js
2. After successful scrape (partial or full), attempt sync:
   ```typescript
   // Sync to EC2 (continue scraper operation even if sync fails)
   try {
     const syncResponse = await syncToEC2(usageData);
     if (syncResponse.success) {
       console.log(`[Auto-Scraper] Synced to EC2 at ${syncResponse.timestamp}`);
       if (VERBOSE) {
         console.log('[Auto-Scraper] EC2 sync successful');
       }
     } else {
       console.warn(`[Auto-Scraper] EC2 sync failed: ${syncResponse.message}`);
     }
   } catch (syncError) {
     // Log but don't crash - scraper continues regardless
     const syncErrorMsg = syncError instanceof Error ? syncError.message : String(syncError);
     console.warn(`[Auto-Scraper] EC2 sync error: ${syncErrorMsg}`);
     if (VERBOSE) {
       console.warn('[Auto-Scraper] Scraper will continue despite sync failure');
     }
   }
   ```
3. Place AFTER retry strategy reset (line 82-84) and success logging (line 87-95)
4. Do NOT use RetryStrategy for sync errors - log and continue

**What to avoid:**
- Don't await sync before resetting retry strategy (sync failures shouldn't affect scraper health)
- Don't exit or stop scraper on sync failures (scraping is primary function)
- Don't retry sync failures here (Phase 34 will add sync-specific retry logic)
- Don't sync before successful scrape confirmation (only sync after data is saved locally)
  </action>
  <verify>
npx tsc --noEmit passes
Auto-scraper imports syncToEC2
Sync attempt is wrapped in try/catch
Scraper continues on sync failure (doesn't exit)
  </verify>
  <done>
auto-scraper.ts calls syncToEC2 after successful scrapes
Sync errors are logged but don't crash scraper
Success/failure messages logged appropriately
Verbose mode shows additional sync details
  </done>
</task>

<task type="auto">
  <name>Task 3: Add environment variable documentation</name>
  <files>.env.example</files>
  <action>
Update the Claude Console Sync section (lines 19-22) to add CLAUDE_SYNC_URL:

```bash
# Claude Console Sync (optional - used for scraper-to-EC2 data sync)
# Generate a secure random key for production (e.g., openssl rand -hex 32)
# Used to authenticate POST requests from laptop scraper to EC2 endpoint
CLAUDE_SYNC_API_KEY=your_secure_api_key_here
# EC2 endpoint URL (defaults to http://localhost:4000/api/claude/console-usage if not set)
CLAUDE_SYNC_URL=https://your-ec2-instance.com/api/claude/console-usage
```

Keep existing comment structure, add URL description after API key.
  </action>
  <verify>
.env.example contains both CLAUDE_SYNC_API_KEY and CLAUDE_SYNC_URL
Comments explain purpose and usage
  </verify>
  <done>
Documentation updated with CLAUDE_SYNC_URL
Clear explanation of when to use custom URL (EC2 deployment)
  </done>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] TypeScript compilation succeeds: npx tsc --noEmit
- [ ] Auto-scraper starts without errors
- [ ] Sync client has unit-testable structure (function exported)
- [ ] All environment variables documented in .env.example
</verification>

<success_criteria>

- sync-client.ts module created with syncToEC2 function
- auto-scraper.ts integrated with sync after successful scrapes
- Sync failures logged but don't crash scraper
- Environment variables documented
- TypeScript compilation passes
- Ready for end-to-end testing in Phase 30 plan 2
  </success_criteria>

<output>
After completion, create `.planning/phases/30-data-sync-client/30-01-SUMMARY.md`:

**Format:**
```markdown
---
phase: 30
plan: 01
subsystem: scraper-sync
type: implementation
requires: [29]
provides: [sync-client]
affects: [31, 34]
tags: [http-client, scraper, ec2-integration]
tech-stack:
  added: [native-fetch, sync-client-module]
  patterns: [post-scrape-sync, error-isolation]
key-decisions:
  - "Sync failures don't crash scraper - scraping is primary function"
  - "No retry logic in sync client - auto-scraper continues on failure"
  - "Use native fetch (Node v22) instead of axios/node-fetch"
key-files:
  - server/claude-scraper/sync-client.ts
  - server/claude-scraper/auto-scraper.ts
---

# Phase 30 Plan 1: Data Sync Client Summary

**Added HTTP client to POST usage data to EC2 after each successful scrape**

## Accomplishments

- Created sync-client.ts module with syncToEC2 function
- Integrated sync into auto-scraper after successful scrapes
- Sync failures isolated - don't crash scraper
- Environment variables documented

## Files Created/Modified

- `server/claude-scraper/sync-client.ts` - HTTP client for EC2 sync
- `server/claude-scraper/auto-scraper.ts` - Added post-scrape sync call
- `.env.example` - Documented CLAUDE_SYNC_URL

## Decisions Made

1. **Sync failure isolation**: Sync errors are logged but don't stop scraper operation. Scraping is the primary function, sync is secondary.
2. **No retry logic in sync client**: Let auto-scraper continue on sync failure. Phase 34 will add comprehensive retry logic.
3. **Native fetch**: Using Node.js v22 native fetch instead of axios/node-fetch (fewer dependencies).

## Issues Encountered

[None expected - straightforward HTTP client integration]

## Next Phase Readiness

**Ready for Phase 31 (EC2 Data Storage):**
- Sync client sending data to EC2
- Need to implement robust storage layer on EC2
- Need to add data retention and versioning

**Testing Required:**
- End-to-end sync flow (scraper â†’ EC2)
- API key authentication
- Network error handling
- Partial data sync validation
```
</output>
