---
phase: 23-error-handling
plan: 02
type: execute
domain: scraper
---

<objective>
Implement graceful degradation for partial data extraction failures. Currently the scraper is all-or-nothing - if any section fails to extract, the entire scrape fails. This plan adds partial success handling where the scraper saves whatever data it can extract and marks missing sections with error details.

**Purpose:** Improve scraper reliability by providing partial data when available rather than failing completely, enabling the UI to show some usage information even during partial Console outages or DOM changes.

**Output:** Scraper that saves partial data with error markers, allowing the frontend to display available metrics while clearly indicating what's missing.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-phase.md
@~/.claude/get-shit-done/templates/summary-template.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/23-error-handling/23-01-SUMMARY.md
@server/claude-scraper/scrape.ts
@server/claude-scraper/auto-scraper.ts

**Tech stack available:**
- Playwright for browser automation
- TypeScript with ESM modules
- Node.js fs for file I/O

**Established patterns from Phase 23-01:**
- RetryStrategy with exponential backoff and circuit breaker
- Error categorization (SESSION_EXPIRED, NETWORK_ERROR, etc.)
- Verbose logging with --verbose flag

**Current state:**
- scrape.ts extracts three data sections: currentSession, allModels, sonnetOnly
- If page.evaluate() returns empty data for any section, the entire result is empty
- No way to distinguish between "section failed to extract" vs "usage is 0%"
- Frontend expects complete data structure or nothing

**Phase 23 goal from roadmap:**
Graceful degradation when partial data is available. Save whatever sections can be extracted, mark missing data with error details.
</context>

<tasks>
<task type="auto">
  <name>Task 1: Add extraction error tracking to ConsoleUsageData interface</name>
  <files>server/claude-scraper/scrape.ts</files>
  <action>
Modify the ConsoleUsageData interface to support partial data:
1. Make all section fields optional (currentSession?, weeklyLimits?)
2. Add extractionErrors field:
   - Type: Record<string, string> where key is section name, value is error message
   - Example: { "currentSession": "DOM selector not found", "allModels": "Timeout waiting for element" }
3. Add isPartial boolean field to indicate if any sections are missing
4. Keep lastUpdated and existing error field for overall scrape failures

This allows the scraper to return whatever data it successfully extracted while documenting what failed.

DO NOT change the structure of individual sections (currentSession, weeklyLimits) - only make them optional at the top level.
  </action>
  <verify>TypeScript compiles. ConsoleUsageData interface has optional section fields, extractionErrors record, and isPartial flag.</verify>
  <done>ConsoleUsageData interface supports partial extraction with error tracking. All sections optional. extractionErrors documents missing sections.</done>
</task>

<task type="auto">
  <name>Task 2: Implement section-level error handling in scrape()</name>
  <files>server/claude-scraper/scrape.ts</files>
  <action>
Refactor page.evaluate() in scrape() to extract sections independently with error handling:
1. Instead of single evaluate() returning all sections, split into three separate extractions:
   - extractCurrentSession(): Try/catch around currentSession extraction
   - extractWeeklyAllModels(): Try/catch around allModels extraction
   - extractWeeklySonnetOnly(): Try/catch around sonnetOnly extraction
2. Each extraction function:
   - Returns section data on success
   - Returns null on failure, logs specific error to extractionErrors
   - Uses timeout for each selector (5 seconds per section)
   - Captures DOM state for debugging if --verbose flag present
3. Combine results into ConsoleUsageData:
   - Include successfully extracted sections
   - Populate extractionErrors for failed sections
   - Set isPartial = true if any section is null
   - Set isPartial = false if all sections extracted
4. Save partial results to usage-data.json even if some sections failed
5. Only throw error if ALL sections failed (truly broken scrape)
6. Log clear message when partial data is saved:
   - "[Scraper] Partial data extracted (2/3 sections). Missing: currentSession"

Keep the overall try/catch for navigation and page load failures - those should still fail the entire scrape.
  </action>
  <verify>Scrape succeeds with partial data when one section fails. usage-data.json contains available sections plus extractionErrors. isPartial flag set correctly. Only fails if all sections fail.</verify>
  <done>Section-level extraction with independent error handling. Saves partial data when available. extractionErrors documents failures. Throws only if all sections fail.</done>
</task>

<task type="auto">
  <name>Task 3: Update auto-scraper to handle partial data results</name>
  <files>server/claude-scraper/auto-scraper.ts</files>
  <action>
Modify auto-scraper.ts to recognize partial success vs complete failure:
1. After scrape completes, check if partial data was saved:
   - Read usage-data.json and check isPartial field
   - Log partial success differently than complete success
   - Show which sections are missing in verbose mode
2. Partial success handling:
   - Do NOT increment consecutiveErrors (partial = success)
   - Reset retry state (like complete success)
   - Log: "[Auto-Scraper] Partial scrape succeeded (2/3 sections). Missing: X"
   - If --verbose: Show extractionErrors details
3. Complete failure handling (unchanged):
   - Increment consecutiveErrors
   - Apply retry strategy
   - Log error category and action
4. Update success log to show completeness:
   - Complete: "[Auto-Scraper] Scrape completed successfully (3/3 sections)"
   - Partial: "[Auto-Scraper] Scrape completed with partial data (2/3 sections)"

This ensures the retry/circuit breaker logic treats partial success as success (resets state) while clearly communicating what's missing.
  </action>
  <verify>Auto-scraper treats partial data as success (resets retry state). Logs show section counts and missing data. Verbose mode shows extractionErrors. Complete failures still trigger retry logic.</verify>
  <done>Auto-scraper handles partial vs complete success. Partial success resets retry state. Logs show data completeness and missing sections. Verbose mode shows extraction errors.</done>
</task>
</tasks>

<verification>
Before declaring plan complete:
- [ ] TypeScript builds without errors (npm run build)
- [ ] ConsoleUsageData interface supports partial data with optional sections
- [ ] scrape.ts extracts sections independently with error handling
- [ ] Partial data saves to usage-data.json with extractionErrors
- [ ] Auto-scraper treats partial success as success (resets retry state)
- [ ] Logs clearly show complete vs partial scrapes
- [ ] Complete failures (all sections fail) still trigger retry logic
- [ ] Verbose mode shows extraction error details
</verification>

<success_criteria>
- All tasks completed
- All verification checks pass
- No TypeScript errors or warnings
- Scraper saves partial data when some sections fail to extract
- Frontend receives whatever data is available (graceful degradation)
- Auto-scraper treats partial success as success (no unnecessary retries)
- Extraction errors are logged and accessible for debugging
- Only complete failures (all sections fail) trigger retry logic
</success_criteria>

<output>
After completion, create `.planning/phases/23-error-handling/23-02-SUMMARY.md` following the standard template:

```markdown
---
phase: 23-error-handling
plan: 02
subsystem: scraper
tags: [error-handling, partial-data, graceful-degradation, resilience]
requires: [23-01]
provides: [partial-data-extraction, section-level-errors, graceful-degradation]
affects: [24, 25]
tech-stack:
  added: []
  patterns: [partial-success-handling, section-independent-extraction]
key-files:
  created: []
  modified:
    - server/claude-scraper/scrape.ts
    - server/claude-scraper/auto-scraper.ts
key-decisions:
  - [List decisions made during implementation]
issues-created: []
metrics:
  duration: [actual duration]
  completed: [completion date]
---

# Phase 23 Plan 2: Graceful Degradation Summary

**[One-line summary of what was accomplished]**

## Accomplishments

[Detailed description of partial data handling]

## Files Created/Modified

[List with line counts and key functionality]

## Technical Details

[Section-level extraction, partial success criteria, error tracking]

## Commits

[List of commits with hashes]

## Decisions Made

[Any implementation choices or tradeoffs]

## Issues Encountered

[Problems and resolutions]

## Next Phase Readiness

Phase 23 complete (2/2 plans finished). Ready for Phase 24: Data Extraction Enhancement

**Note for Phase 24:** The partial data structure is now in place. Phase 24 can add new data sections (billing, plan details, historical data) using the same section-independent extraction pattern to maintain graceful degradation.
```
</output>
